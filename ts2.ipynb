{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/h/DIP_Project/freq-net\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1624800 parameters.\n"
     ]
    }
   ],
   "source": [
    "from freq_net.model.model import FreqNet\n",
    "\n",
    "model = FreqNet(n_resgroups=2 , n_depthwise_resgroups=1 , n_resblocks = 3)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"The model has {num_params} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [7.90193261632736e-05]\n",
      "Epoch: 1 LR: [7.660823673347575e-05]\n",
      "Epoch: 2 LR: [7.332055025267214e-05]\n",
      "Epoch: 3 LR: [6.923676427917746e-05]\n",
      "Epoch: 4 LR: [6.445695860172482e-05]\n",
      "Epoch: 5 LR: [5.909831709737849e-05]\n",
      "Epoch: 6 LR: [5.329222507748192e-05]\n",
      "Epoch: 7 LR: [4.718101146477996e-05]\n",
      "Epoch: 8 LR: [4.09144127030451e-05]\n",
      "Epoch: 9 LR: [3.4645840000202857e-05]\n",
      "Epoch: 10 LR: [2.8528532330289877e-05]\n",
      "Epoch: 11 LR: [2.2711672592470017e-05]\n",
      "Epoch: 12 LR: [1.733652905977669e-05]\n",
      "Epoch: 13 LR: [1.2532647922707756e-05]\n",
      "Epoch: 14 LR: [8.414035832668668e-06]\n",
      "Epoch: 15 LR: [5.0750438674695244e-06]\n",
      "Epoch: 16 LR: [2.584980955966816e-06]\n",
      "Epoch: 17 LR: [9.780232940015221e-07]\n",
      "Epoch: 18 LR: [2.2372432417573743e-07]\n",
      "Epoch: 19 LR: [1e-07]\n",
      "Epoch: 20 LR: [1.083701586448492e-06]\n",
      "Epoch: 21 LR: [7.873022913467656e-06]\n",
      "Epoch: 22 LR: [9.796676574016854e-06]\n",
      "Epoch: 23 LR: [1.3469209498370389e-05]\n",
      "Epoch: 24 LR: [1.804488557753268e-05]\n",
      "Epoch: 25 LR: [2.32768557383891e-05]\n",
      "Epoch: 26 LR: [2.8993067806294252e-05]\n",
      "Epoch: 27 LR: [3.5034241396355525e-05]\n",
      "Epoch: 28 LR: [4.124215848850557e-05]\n",
      "Epoch: 29 LR: [4.7458501911656746e-05]\n",
      "Epoch: 30 LR: [5.352675984371873e-05]\n",
      "Epoch: 31 LR: [5.9295177467784324e-05]\n",
      "Epoch: 32 LR: [6.462004128103473e-05]\n",
      "Epoch: 33 LR: [6.936897198679791e-05]\n",
      "Epoch: 34 LR: [7.342404155329677e-05]\n",
      "Epoch: 35 LR: [7.668458964255684e-05]\n",
      "Epoch: 36 LR: [7.906964602400146e-05]\n",
      "Epoch: 37 LR: [8.051988653629508e-05]\n",
      "Epoch: 38 LR: [8.099906725319558e-05]\n",
      "Epoch: 39 LR: [8.04948972967029e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "d:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cossine_lr(optimizer, t_max, eta_min):\n",
    "    \"\"\"\n",
    "    eta_max will be the learning rate of optimizer\n",
    "    \"\"\"\n",
    "    return CosineAnnealingLR(optimizer=optimizer, T_max=t_max, eta_min=eta_min)\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 8e-5 , betas = (0.9 , 0.99) , eps = 1e-8)\n",
    "\n",
    "# Define the CosLR scheduler\n",
    "scheduler = cossine_lr(optimizer,  t_max = 20 , eta_min = 1e-7)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(40):\n",
    "    # Train the model for one epoch\n",
    "    train_loss = ...\n",
    "\n",
    "    # Update the learning rate using the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print the current learning rate\n",
    "    print(\"Epoch:\", epoch, \"LR:\", scheduler.get_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "dropout_rate1 = 0.5\n",
    "dropout_rate2 = 0.3\n",
    "\n",
    "# Create the convolutional layers and activation function\n",
    "l1 = nn.Linear(4 , 10)\n",
    "\n",
    "conv1 = nn.Conv2d(1, 3, 3)\n",
    "act = nn.ReLU()\n",
    "conv2 = nn.Conv2d(3, 6, 3)\n",
    "\n",
    "# Create the dropout layers\n",
    "dropout1 = nn.Dropout(dropout_rate1)\n",
    "dropout2 = nn.Dropout(dropout_rate2)\n",
    "\n",
    "# Create the sequential module with dropouts\n",
    "body = nn.Sequential(\n",
    "    conv1,\n",
    "    act,\n",
    "    dropout1,\n",
    "    conv2,\n",
    "    dropout2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7138,  0.5866, -0.0728, -0.4723,  3.2172,  2.2049, -3.7183,  0.7901,\n",
      "        -2.8355,  2.8245], grad_fn=<AddBackward0>)\n",
      "\n",
      "tensor([-0.3271,  1.8931,  2.0541, -2.0536,  2.7993,  1.6926,  0.4209, -0.7130,\n",
      "        -0.5047, -3.4515], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "l1 = nn.Linear(2 , 10)\n",
    "l2 = nn.Linear(2 , 10)\n",
    "ol1 = l1(torch.tensor([1 , 2 , 3 , 4]).to(torch.float32))\n",
    "ol2 = l2(torch.tensor([1 , 2 , 3 , 4]).to(torch.float32))\n",
    "print(ol1)\n",
    "print()\n",
    "print(ol2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 4., 6.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout1(torch.tensor([1.0 , 2.0 , 3.0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
